# HoloDub (å¹»è¯»)

> Dub the whole performance, not just the words.

HoloDub is a **self-hosted video translation & dubbing toolkit for creators**.

It is designed for people who need to re-voice videos for platforms like **YouTube, Bilibili, TikTok** and beyond:
- channel owners,
- clip / highlights makers,
- fansub groups,
- small studios that want to keep everything on their own GPU box or cloud instance.

Instead of just â€œtranslating subtitles and dropping a generic voice on topâ€, HoloDub rebuilds the audio track **around the original timeline** using:

> **Smart semantic splitting + duration-aware TTS**, so dubbed audio stays in sync with the video.

---

## âœ¨ Features at a glance

### ğŸ¬ Timeline-first dubbing

- **Smart semantic split**
  - Whisper-based ASR with **word-level timestamps**.
  - VAD-assisted pause detection (Pyannote).
  - Splits by **meaning and natural pauses**, not fixed-length chunks.
  - Configurable segment length window (e.g. 2â€“15 seconds).

- **Duration-aware TTS (IndexTTS2)**
  - Every segment carries its **original duration** as metadata.
  - IndexTTS2 uses this as a hard constraint: generated speech length â‰ˆ original segment length.
  - Optional `atempo` / light time-stretching as fallback when needed.

### ğŸ—£ Multi-speaker & custom voices

- **Speaker diarization**
  - Automatically clusters different speakers (e.g. `SPK_01`, `SPK_02`, â€¦).
  - Each segment is tagged with a speaker ID.

- **Custom voice profiles**
  - `sample` mode  
    Upload 1â€“N reference clips and use zero-shot voice cloning.
  - `checkpoint` mode  
    Load your own SoVITS / IndexTTS-style checkpoints  
    (`.pth/.ckpt + .index + config`).
  - Language tags, internal speaker IDs and model paths are stored centrally.

- **Speaker â†’ voice mapping**
  - For each job, map `SPK_01 / SPK_02 / â€¦` to different voice profiles.
  - Use different timbres in one video (host, guest, narratorâ€¦).
  - Change mappings and re-run only the affected segments.

### ğŸš€ Creator-friendly & self-hosted

- **Run it where you want**
  - On your own GPU PC at home.
  - On a single cloud GPU instance.
  - No external SaaS required by default.

- **Single-node by default**
  - Simple â€œone-boxâ€ layout:
    - Go control plane (API + worker)
    - Python ML service (GPU)
    - PostgreSQL + Redis
  - All wired together with Docker Compose.

- **Keep your data**
  - Videos, audio, transcripts and models live under a shared `/data` volume.
  - Database stores **relative paths** only (easier to move / backup).

---

## ğŸ§± Architecture (short version)

You donâ€™t need to understand all of this to *use* HoloDub â€”  
but if you want to hack on it, hereâ€™s the rough picture.

### Control plane (Go)

- Go 1.22+, Gin, GORM.
- Drives jobs through stages:

  `media` â†’ `separate` â†’ `asr_smart` â†’ `translate` â†’ `tts_duration` â†’ `merge`

- Stores:
  - `jobs` (one per video),
  - `voice_profiles` (custom voices),
  - `speakers` (logical speakers per job),
  - `speaker_voice_bindings` (who uses which voice),
  - `segments` (time-aligned pieces).

- Uses Redis as task queue:
  - Job-level stage tasks (e.g. `job:123:stage:asr_smart`).
  - Optionally segment-level TTS tasks.

- Talks to an LLM (Qwen / DeepSeek / etc.) for translation:
  - Prompts tuned for **dubbing-friendly, length-aware** output.

### Data plane (Python / GPU)

- Python 3.10+, FastAPI, PyTorch.
- Global GPU lock / semaphore to avoid OOM.
- Model registry with simple LRU-style caching:
  - Demucs / UVR5 â€” vocal & BGM separation.
  - Faster-Whisper â€” ASR with word-level timestamps.
  - Pyannote.audio â€” VAD and (optional) diarization.
  - IndexTTS2 â€” duration-aware TTS.

- HTTP endpoints (examples):
  - `POST /asr/smart_split`  
    â†’ segments with `start_ms`, `end_ms`, `text`, `speaker_label`, `split_reason`.
  - `POST /tts/run`  
    â†’ takes `text` + `target_duration_sec` + `voice_config` + `output_relpath`,  
    â†’ returns saved audio path + actual duration.

### Shared storage

- Host `./data` mounted as `/data` in all containers.
- DB only stores **relative paths** (e.g. `jobs/101/input.mp4`).
- Apps resolve absolute paths using `DATA_ROOT`.

---

## ğŸ—‚ Data model highlights

- **jobs**
  - One row per video.
  - Tracks status, current stage, progress, config JSON, IO paths, errors, retries, heartbeat.

- **voice_profiles**
  - Describes how to synthesize a voice:
    - sample-based or checkpoint-based,
    - paths to models / indexes / configs,
    - language tags, internal speaker IDs.

- **speakers**
  - Per-job logical speakers generated by diarization  
    (e.g. `SPK_01` is â€œthe hostâ€, `SPK_02` is â€œthe guestâ€).

- **speaker_voice_bindings**
  - Maps `speaker_id` â†’ `voice_profile_id`.  
    This is where you say: *â€œfor this job, SPK_01 uses Voice A, SPK_02 uses Voice B.â€*

- **segments**
  - Minimal units for translation & TTS.
  - Carry:
    - `start_ms`, `end_ms`
    - `original_duration_ms` (generated column)
    - `src_text`, `tgt_text`
    - `tts_audio_path`, `tts_duration_ms`
    - `split_reason` (by punctuation, silence, max-duration, â€¦)

---

## ğŸ“Š Status

HoloDub is in **early design & prototyping**.

- [x] Architecture & schema design
- [x] Speaker / voice mapping model
- [ ] Go control plane skeleton (API + worker)
- [ ] Python ML service skeleton (FastAPI)
- [ ] Smart split implementation (ASR + VAD + rules)
- [ ] Duration-aware TTS integration (IndexTTS2)
- [ ] End-to-end demo pipeline

If youâ€™re interested in hacking on it, PRs and discussions are very welcome once the initial skeleton lands.

---

## ğŸ›  Tech stack

- **Control plane**: Go, Gin, GORM, Redis, PostgreSQL  
- **ML service**: FastAPI, PyTorch, Demucs/UVR5, Faster-Whisper, Pyannote, IndexTTS2  
- **Orchestration**: Docker Compose  
- **Translation**: Qwen / DeepSeek / pluggable LLM providers  

---

## ğŸ“œ License

Apache 2.0
